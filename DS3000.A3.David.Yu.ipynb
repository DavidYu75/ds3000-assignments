{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 3000 - Assignment 3\n",
    "\n",
    "**Student Name**: [David Yu]\n",
    "\n",
    "**Date**: [1/22/25]\n",
    "\n",
    "\n",
    "### Submission Instructions\n",
    "Submit this `ipynb` file to canvas.\n",
    "\n",
    "The `ipynb` format stores outputs from the last time you ran the notebook.  (When you open a notebook it has the figures and outputs of the last time you ran it too).  To ensure that your submitted `ipynb` file represents your latest code, make sure to give a fresh run `Kernel > Restart & Run All` just before uploading the `ipynb` file to Canvas.\n",
    "\n",
    "### Academic Integrity\n",
    "\n",
    "**Writing your homework is an individual effort.**  You may discuss general python problems with other students but under no circumstances should you observe another student's code which was written for this assignment, from this year or past years.  Pop into office hours or DM us in MS Teams if you have a specific question about your work or if you would like another pair of eyes or talk through your code.\n",
    "\n",
    "Don't forget to cite websites which helped you solve a problem in a unique way.  You can do this in markdown near the code or with a simple one-line comment. You do not need to cite the official python documentation.\n",
    "\n",
    "**Documentation / style counts for credit**  Please refer to the Pep-8 style, to improve the readability and consistency of your Python code. For more information, read the following article [How to Write Beautiful Python Code With PEP 8](https://realpython.com/python-pep8/) or ask your TA's for tips.\n",
    "\n",
    "**NOTE:<span style='color:red'> Write python expressions to answer ALL questions below and ensure that you use the `print()` function to display the output.</span>** Each question should be answered in a new code cell. For example, your solution for question 1.1 should be in a different code cell from your solution for question 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Problem (50 pts)\n",
    "### Instructions\n",
    "\n",
    "You are hired as a Data Scientist for the NYC Taxi and Limousine Commission (TLC) and this company routinely collects trip data from all licensed cabs in NY. Your team would like to use this data in the future to understand how New Yorkers use these taxis. Your first assignment is to **analyze the data of the Green Taxis, and determine if its a viable source.**\n",
    "\n",
    "Your supervisor has requested that you perform your analysis using data from December 2023 and would like for you to present your findings during the next team meeting. Use a Jupyter notebook to explain your data wrangling and analytical steps to your colleagues. \n",
    "\n",
    "**Data & Documentation**\n",
    "\n",
    "- **Data Source:** https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-12.parquet\n",
    "- **Data Dictionary:** [Green Trips Data Dictionary](https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf).\n",
    "\n",
    "The TLC trip record data and a description of the initiative can be found at the following link: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "**`Note:`** Ensure that you explain the results from each question.\n",
    "\n",
    "\n",
    "**In this assignment you will use the Pandas library. Pandas allows us to load data that's stored in a various formats, e.g. excel, csv, txt, parquet, etc. In this assignment, you will learn to read data in parquet format which is one of the preferred formats when working with `Big Data`. [Click here to learn more about parquet files](https://towardsdatascience.com/csv-files-for-storage-no-thanks-theres-a-better-option-72c78a414d1d)**\n",
    "\n",
    "I also provided starter code below to help you to read the data in parquet format. First, you'll need to install a new library called *fastparquet*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastparquet in /opt/homebrew/lib/python3.11/site-packages (2024.11.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/homebrew/lib/python3.11/site-packages (from fastparquet) (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from fastparquet) (2.2.1)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/homebrew/lib/python3.11/site-packages (from fastparquet) (2.9.1)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from fastparquet) (2024.12.0)\n",
      "Requirement already satisfied: packaging in /Users/davidyu/Library/Python/3.11/lib/python/site-packages (from fastparquet) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/davidyu/Library/Python/3.11/lib/python/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas>=1.5.0->fastparquet) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/davidyu/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#run this cell to install the fast parquet library\n",
    "%pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data\n",
    "Load the NYC Green Taxi Trip Records data directly from the URL into a data frame called tripdata_df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's load the data into a dataframe\n",
    "\n",
    "tripdata_df = pd.read_parquet(path = 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-12.parquet', engine = 'fastparquet')\n",
    "\n",
    "#Note: you'll notice two new things. 1) We are using a new function called `read_parquet()` which indicates that we are \n",
    "#      reading data in parquet format. 2) We used the fastparquet library as the engine. \n",
    "#      That's it! Pandas will load the data into a dataframe. You can now use the dataframe for EDA.\n",
    "\n",
    "# Reminder: How does Pandas read other file formats?\n",
    "# 1. Excel files: pd.read_excel()\n",
    "# 2. CSV Files: pd.read_csv()\n",
    "# ... and now you know how to read parquet files using pd.read_parquet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect the Data (5 pts) \n",
    "Inspect the data and perform the following: \n",
    "1. display the dimensions, and indicate if the variables have suitable types.\n",
    "2. convert all variables to suitable types\n",
    "\n",
    "*Tip*:  Think about whether the data type is accurate for the field. Refer to the data dictionary and review sample values in the dataframe before converting types. **Demonstrate ALL steps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Dimensions:\n",
      "Rows: 64215\n",
      "Columns: 20\n",
      "\n",
      "Initial Data Types:\n",
      "VendorID                        float64\n",
      "lpep_pickup_datetime     datetime64[us]\n",
      "lpep_dropoff_datetime    datetime64[us]\n",
      "store_and_fwd_flag               object\n",
      "RatecodeID                      float64\n",
      "PULocationID                    float64\n",
      "DOLocationID                    float64\n",
      "passenger_count                 float64\n",
      "trip_distance                   float64\n",
      "fare_amount                     float64\n",
      "extra                           float64\n",
      "mta_tax                         float64\n",
      "tip_amount                      float64\n",
      "tolls_amount                    float64\n",
      "ehail_fee                       float64\n",
      "improvement_surcharge           float64\n",
      "total_amount                    float64\n",
      "payment_type                    float64\n",
      "trip_type                       float64\n",
      "congestion_surcharge            float64\n",
      "dtype: object\n",
      "Suitable types:\n",
      "- lpep_pickup_datetime, lpep_dropoff_datetime (datetime64[us]): Suitable for timestamps\n",
      "- trip_distance (float64): Suitable for distances with decimals\n",
      "- fare_amount, extra, mta_tax, tip_amount, tolls_amount, ehail_fee, improvement_surcharge, total_amount, congestion_surcharge (float64): Suitable for money amount with cents\n",
      "\n",
      "Needs conversion:\n",
      "- ID columns (float64): Should be Int64 for categorical identifiers\n",
      "- passenger_count (float64): Should be Int64 for whole numbers\n",
      "- store_and_fwd_flag (object): Should be category\n",
      "- payment_type, trip_type (float64): Should be category\n"
     ]
    }
   ],
   "source": [
    "# 1. display the dimensions, and indicate if the variables have suitable types.\n",
    "print(\"DataFrame Dimensions:\")\n",
    "print(f\"Rows: {tripdata_df.shape[0]}\")\n",
    "print(f\"Columns: {tripdata_df.shape[1]}\")\n",
    "\n",
    "print(\"\\nInitial Data Types:\")\n",
    "print(tripdata_df.dtypes)\n",
    "\n",
    "print(\"Suitable types:\")\n",
    "print(\"- lpep_pickup_datetime, lpep_dropoff_datetime (datetime64[us]): Suitable for timestamps\")\n",
    "print(\"- trip_distance (float64): Suitable for distances with decimals\")\n",
    "print(\"- fare_amount, extra, mta_tax, tip_amount, tolls_amount, ehail_fee, improvement_surcharge, total_amount, congestion_surcharge (float64): Suitable for money amount with cents\")\n",
    "print(\"\\nNeeds conversion:\")\n",
    "print(\"- ID columns (float64): Should be Int64 for categorical identifiers\")\n",
    "print(\"- passenger_count (float64): Should be Int64 for whole numbers\")\n",
    "print(\"- store_and_fwd_flag (object): Should be category\")\n",
    "print(\"- payment_type, trip_type (float64): Should be category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converted Data Types:\n",
      "VendorID                          Int64\n",
      "lpep_pickup_datetime     datetime64[us]\n",
      "lpep_dropoff_datetime    datetime64[us]\n",
      "store_and_fwd_flag             category\n",
      "RatecodeID                        Int64\n",
      "PULocationID                      Int64\n",
      "DOLocationID                      Int64\n",
      "passenger_count                   Int64\n",
      "trip_distance                   float64\n",
      "fare_amount                     float64\n",
      "extra                           float64\n",
      "mta_tax                         float64\n",
      "tip_amount                      float64\n",
      "tolls_amount                    float64\n",
      "ehail_fee                       float64\n",
      "improvement_surcharge           float64\n",
      "total_amount                    float64\n",
      "payment_type                   category\n",
      "trip_type                      category\n",
      "congestion_surcharge            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 2. convert all variables to suitable types\n",
    "# Integer columns\n",
    "int_columns = ['VendorID', 'RatecodeID', 'PULocationID', 'DOLocationID', 'passenger_count']\n",
    "for col in int_columns:\n",
    "    tripdata_df[col] = tripdata_df[col].astype('Int64')\n",
    "\n",
    "# Categorical columns\n",
    "cat_columns = ['store_and_fwd_flag', 'payment_type', 'trip_type']\n",
    "for col in cat_columns:\n",
    "    tripdata_df[col] = tripdata_df[col].astype('category')\n",
    "\n",
    "print(\"\\nConverted Data Types:\")\n",
    "print(tripdata_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Types of Data (2 pts) \n",
    "Are there any categorical data in this dataset. If they exist, **demonstrate** at least two categorical variables and indicate if they are **nominal** or **ordinal** (hint: read the data dictionary above which describes all fields in the data)\n",
    "\n",
    "*Tip*: Justify why you classified each field as categorical. Provide your reasoning and the evidence to support your decision based on values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical data does exist in this dataset.\n",
      "store_and_fwd_flag:\n",
      "['N', 'Y', NaN]\n",
      "Categories (2, object): ['N', 'Y']\n",
      "store_and_fwd_flag is categorical data. It is nominal as it only takes in two values, 'Y' and 'N'. It represents whether the trip record was stored in vehicle memory before sending to the server.\n",
      "\n",
      "payment_type unique values:\n",
      "[1.0, 2.0, 3.0, 4.0, NaN]\n",
      "Categories (4, float64): [1.0, 2.0, 3.0, 4.0]\n",
      "payment_type is categorical data. It is nominal as it only takes in four values, 1, 2, 3, and 4. It represents the payment method used for the trip.\n"
     ]
    }
   ],
   "source": [
    "print(\"Categorical data does exist in this dataset.\")\n",
    "\n",
    "print(\"store_and_fwd_flag:\")\n",
    "print(tripdata_df['store_and_fwd_flag'].unique())\n",
    "print(\"store_and_fwd_flag is categorical data. It is nominal as it only takes in two values, 'Y' and 'N'. It represents whether the trip record was stored in vehicle memory before sending to the server.\")\n",
    "\n",
    "print(\"\\npayment_type unique values:\")\n",
    "print(tripdata_df['payment_type'].unique())\n",
    "print(\"payment_type is categorical data. It is nominal as it only takes in four values, 1, 2, 3, and 4. It represents the payment method used for the trip.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Missing Values (10 pts) \n",
    "Evaluate the frequency of missing values in the dataset and perform the following:\n",
    "1. explain if missing values are an isolated case or a widespread issue? \n",
    "2. what method would you recommend to handle missing data?\n",
    "3. write the suitable code to handle the missing values.\n",
    "\n",
    "*Tip*: When handling missing values, evaluate the row-wise and column-wise data to make informed decisions regarding which approach is suitable. When in doubt, review the Canvas lectures & readings for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values\n",
      "\n",
      "Absolute count of missing values:\n",
      "store_and_fwd_flag       4912\n",
      "RatecodeID               4912\n",
      "passenger_count          4912\n",
      "ehail_fee               64215\n",
      "payment_type             4912\n",
      "trip_type                4914\n",
      "congestion_surcharge     4912\n",
      "dtype: int64\n",
      "\n",
      "Percentage of missing values:\n",
      "store_and_fwd_flag        7.649303\n",
      "RatecodeID                7.649303\n",
      "passenger_count           7.649303\n",
      "ehail_fee               100.000000\n",
      "payment_type              7.649303\n",
      "trip_type                 7.652418\n",
      "congestion_surcharge      7.649303\n",
      "dtype: float64\n",
      "\n",
      "Rows with at least one missing value: 64215\n",
      "Percentage of rows with missing values: 100.00%\n",
      "\n",
      "This is a widespread issue because:\n",
      "1. All rows have at least one missing value\n",
      "2. ehail_fee column has 100% missing values\n",
      "3. Seven other columns have ~7.65% missing values each:\n",
      "- store_and_fwd_flag\n",
      "- RatecodeID\n",
      "- passenger_count\n",
      "- payment_type\n",
      "- trip_type\n",
      "- congestion_surcharge\n",
      "\n",
      "Recommended method to handle missing data:\n",
      "1. For ehail_fee (100% missing), we can drop this column as it contains no information.\n",
      "\n",
      "2. For columns with ~7.65% missing values, we cab:\n",
      "- Numeric columns (passenger_count, congestion_surcharge): Use median imputation\n",
      "- Categorical columns: Create a 'Missing' category\n",
      "\n",
      "Handling missing values:\n",
      "\n",
      "Remaining missing values after cleaning:\n",
      "store_and_fwd_flag     4912\n",
      "RatecodeID            64215\n",
      "payment_type          64215\n",
      "trip_type             64215\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. explain if missing values are an isolated case or a widespread issue?\n",
    "missing_values = tripdata_df.isnull().sum()\n",
    "missing_percentages = (tripdata_df.isnull().sum() / len(tripdata_df)) * 100\n",
    "\n",
    "print(\"Missing Values\")\n",
    "print(\"\\nAbsolute count of missing values:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print(missing_percentages[missing_percentages > 0])\n",
    "\n",
    "# Check rows with missing values\n",
    "rows_with_missing = tripdata_df.isnull().any(axis=1).sum()\n",
    "print(f\"\\nRows with at least one missing value: {rows_with_missing}\")\n",
    "print(f\"Percentage of rows with missing values: {(rows_with_missing/len(tripdata_df))*100:.2f}%\")\n",
    "\n",
    "print(\"\\nThis is a widespread issue because:\")\n",
    "print(\"1. All rows have at least one missing value\")\n",
    "print(\"2. ehail_fee column has 100% missing values\")\n",
    "print(\"3. Seven other columns have ~7.65% missing values each:\")\n",
    "print(\"- store_and_fwd_flag\")\n",
    "print(\"- RatecodeID\")\n",
    "print(\"- passenger_count\")\n",
    "print(\"- payment_type\")\n",
    "print(\"- trip_type\")\n",
    "print(\"- congestion_surcharge\")\n",
    "\n",
    "# 2. what method would you recommend to handle missing data?\n",
    "print(\"\\nRecommended method to handle missing data:\")\n",
    "print(\"1. For ehail_fee (100% missing), we can drop this column as it contains no information.\")\n",
    "print(\"\\n2. For columns with ~7.65% missing values, we cab:\")\n",
    "print(\"- Numeric columns (passenger_count, congestion_surcharge): Use median imputation\")\n",
    "print(\"- Categorical columns: Create a 'Missing' category\")\n",
    "\n",
    "# 3. Code to handle missing values\n",
    "print(\"\\nHandling missing values:\")\n",
    "\n",
    "# Create a copy for cleaning\n",
    "cleaned_df = tripdata_df.copy()\n",
    "\n",
    "# Drop ehail_fee column (100% missing)\n",
    "cleaned_df = cleaned_df.drop('ehail_fee', axis=1)\n",
    "\n",
    "# Handle numeric columns with median imputation\n",
    "numeric_cols = ['passenger_count', 'congestion_surcharge']\n",
    "for col in numeric_cols:\n",
    "   if col in cleaned_df.columns:\n",
    "       median_value = cleaned_df[col].median()\n",
    "       cleaned_df[col] = cleaned_df[col].fillna(median_value)\n",
    "\n",
    "# Handle categorical columns\n",
    "categorical_cols = ['store_and_fwd_flag', 'payment_type', 'trip_type', 'RatecodeID']\n",
    "for col in categorical_cols:\n",
    "   if col in cleaned_df.columns:\n",
    "       existing_categories = cleaned_df[col].dropna().unique()\n",
    "       new_categories = list(existing_categories) + ['Missing']\n",
    "       cleaned_df[col] = cleaned_df[col].astype(str).fillna('Missing')\n",
    "       cleaned_df[col] = pd.Categorical(cleaned_df[col], categories=new_categories)\n",
    "\n",
    "print(\"\\nRemaining missing values after cleaning:\")\n",
    "print(cleaned_df.isnull().sum()[cleaned_df.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Invalid Data (10 pts) \n",
    "Explore the data to determine if there are any invalid values and perform the following: \n",
    "1. demonstrate at least three issues that you found. **`NOTE`: The issues that you found should not include any aspects from the previous questions.**\n",
    "2. write suitable code to handle the invalid data that you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Answer the following questions using the prepared data.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Visualization (12 pts) \n",
    "Create bar charts that show the following: \n",
    "1. the most common way that New Yorkers request/hail a cab. \n",
    "2. the data **for each week** to determine if there are patterns in the frequency of trips based on the day of the week (i.e. Sun to Sat).\n",
    "3. the most common pick-up locations (display the top 5 locations). \n",
    "4. the most common drop-off locations (display the top 5 locations). \n",
    "\n",
    "Explain each chart.\n",
    "\n",
    "*`TIP`: If the answers from your analysis in this question contains erroneous or invalid numeric values, this means that the data needs to be cleaned to ensure that the analytical results are coherent.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Analysis (9 pts)\n",
    "Write suitable code to answer the following:\n",
    "1. Calculate the **range** of distances traveled by the green taxis. Then, compare the average distance with the shortest and longest distances. What insights can be drawn about travel using these taxis?\n",
    "2. What is the most expensive and least expensive trip? \n",
    "3. What is the minimum, average and maximum **tip amount** for all trips that are 1) under 2 miles and 2) over 2 miles (i.e. >= 2)? \n",
    "\n",
    "Explain the results.\n",
    "\n",
    "*`TIP`: If the answers from your analysis in this question contains erroneous or invalid numeric values, this means that the data needs to be cleaned to ensure that the analytical results are coherent.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Findings (2 pts)\n",
    "Based on the preliminary analysis, is this data a viable source that can be used by your team in the future to understand how New Yorkers use these taxis? Justify your response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
